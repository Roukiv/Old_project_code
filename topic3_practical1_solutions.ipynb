{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# University of Sheffield - School of Mathematics and Statistics\n",
    "## MAS316/MAS414/MAS6446: Mathematical Modelling of Natural Systems\n",
    "João Carreiras - G39e - j.carreiras@sheffield.ac.uk\n",
    "### Topic III: Machine learning methods to retrieve forest biophysical parameters\n",
    "### Practical #1: Predicting the age of secondary forests with decision trees, bagging and boosting methods \n",
    "\n",
    "### <span style=\"color:red\">*Confidentiality: The contents of this Jupyter Notebook are not to be shared outside this course*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context and objectives\n",
    "In the tropics, deforestation typically results in the replacement of old-growth forests by croplands and pastures but these are often abandoned after a few years and replaced with naturally regrowing forests - secondary forests. These secondary forests have different ages, and accumulate aboveground biomass and restore biodiversity lost previously during the initial deforestation process.\n",
    "\n",
    "In this practical session we'll be using a dataset containing the age of secondary forests in the Amazon. The objective is to test several machine learning algorithms (decision trees, bagging, boosting) to estimate the age of secondary forests with several predictors obtained from satellite observations. We will assess the performance of each algorithm by computing the error obtained with an independent sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Landsat 5 Thematic Mapper (TM) is a satellite sensor that acquired observations over the Earth surface between 1984 and 2013. The Landsat 5 satellite orbited the the Earth in a sun-synchronous, near-polar orbit, at an altitude of 705 km (438 mi), inclined at 98.2 degrees, and circled the Earth every 99 minutes. The TM sensor onboard Landsat 5 is an optical sensor, acquiring observations in seven spectral bands:\n",
    "* Band 1: visible (0.45 - 0.52 µm)\n",
    "* Band 2: visible (0.52 - 0.60 µm)\n",
    "* Band 3: visible (0.63 - 0.69 µm)\n",
    "* Band 4: near-infrared (0.76 - 0.90 µm)\n",
    "* Band 5: near-infrared (1.55 - 1.75 µm)\n",
    "* Band 6: thermal (10.40 - 12.50 µm) - we won't be using this band\n",
    "* Band 7: mid-infrared (2.08 - 2.35 µm)\n",
    "\n",
    "Advanced Land Observing Satellite (ALOS) Phased Arrayed L-band Synthetic Aperture Radar (PALSAR) yielded detailed, all-weather, day-and-night observation of the Earth's surface between 2006 and 2011. Data acquired by the PALSAR sensor are from multiple observation modes with variable polarization, resolution, swath width, and off-nadir angle. In this exercise we will use the following variables:\n",
    "* HH: backscatter intensity in the HH polarisation\n",
    "* HV: backscatter intensity in the HV polarisation\n",
    "\n",
    "The dataset has 1,876 observations of the age of secondary forests and eight predictors. The predictors were obtained from ALOS PALSAR and Landsat 5 TM; they are all adimensional.\n",
    "\n",
    "* *ID*: code assigned to each observation\n",
    "* *ASF*: age of secondary forests (years)\n",
    "* *HH*: ALOS PALSAR backscatter intensity - HH polarisation\n",
    "* *HV*: ALOS PALSAR backscatter intensity - HV polarisation\n",
    "* *b1*: Landsat 5 TM surface reflectance - Band 1\n",
    "* *b2*: Landsat 5 TM surface reflectance - Band 2\n",
    "* *b3*: Landsat 5 TM surface reflectance - Band 3\n",
    "* *b4*: Landsat 5 TM surface reflectance - Band 4\n",
    "* *b5*: Landsat 5 TM surface reflectance - Band 5\n",
    "* *b7*: Landsat 5 TM surface reflectance - Band 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data\n",
    "Throughout this practical session we will use several Python libraries: pandas, numpy, mathplotlib, sklearn; [sklearn](https://scikit-learn.org/stable) offers numerous tools for predictive data analysis, hosting implementations of the most important machine learning tools we'll be using during this topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first read the dataset containing the observations from a csv file on a shared google drive. Those not being able to access google, please download the offline file in Blackboard (asf_amazon.csv), save it to your computer, and change the path in the code below to have it read into the pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "asf_amazon = pd.read_csv(\"https://drive.google.com/uc?export=download&id=1gYcYOTEHr7d-zh9q-H2i-Gy0leuQUMf3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check descriptive statistics\n",
    "asf_amazon.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response, predictors and partitioning the dataset into training and testing subsets\n",
    "In this exercise we will test several machine learning algorithms to estimate the age of secondary forests as a function of several measurements:\n",
    "\n",
    "*ASF* = f(*HH*, *HV*, *b1*, *b2*, *b3*, *b4*, *b5*, *b7*)\n",
    "\n",
    "We will use a 75:25 random partition of the dataset to obtain the training and testing subsets, respectively. The training subset will be used for model fitting, whereas the testing subset will be kept aside and only used when assessing the predictive capability of the models. <span style=\"color:red\">Please note, we need to set a random number so that we can reproduce the results</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = asf_amazon.ASF ## set the response\n",
    "X = asf_amazon[['HH','HV','b1','b2','b3','b4','b5','b7']] ## set the predictors\n",
    "## Split into train and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting regression trees\n",
    "The scikit-learn library has a specific function to fit a regression tree based on the CART algorithm (see Lecture Notes). The function is called [DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor), and in the link you can learn about the parameters controlling this function.\n",
    "\n",
    "In the code below, we'll specify the cost-complexity pruning approach to select the best tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specifying the full tree (according to defaults)\n",
    "dtr_full = DecisionTreeRegressor(random_state=1234)\n",
    "##\n",
    "## Fitting the full tree\n",
    "dtr_full.fit(X_train, y_train)\n",
    "##\n",
    "## Plot the tree\n",
    "tree.plot_tree(dtr_full, feature_names=X_train.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predicting to the train and test subsets\n",
    "y_train_pred_full = dtr_full.predict(X_train)\n",
    "y_test_pred_full  = dtr_full.predict(X_test)\n",
    "##\n",
    "## bias-variance decomposition of the error\n",
    "dtr_full_train_rmse = np.sqrt(metrics.mean_squared_error(y_train, y_train_pred_full))\n",
    "dtr_full_train_bias = np.mean(y_train - y_train_pred_full)\n",
    "dtr_full_train_var  = np.var(y_train - y_train_pred_full)\n",
    "print(dtr_full_train_rmse, dtr_full_train_bias, dtr_full_train_var)\n",
    "dtr_full_test_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_test_pred_full))\n",
    "dtr_full_test_bias = np.mean(y_test - y_test_pred_full)\n",
    "dtr_full_test_var  = np.var(y_test - y_test_pred_full)\n",
    "print(dtr_full_test_rmse, dtr_full_test_bias, dtr_full_test_var)\n",
    "##\n",
    "## Can you comment on the results?\n",
    "## The full tree perfectly fits the data - overfitting\n",
    "## BUT when applied to the test subset, the errors are quite high\n",
    "##\n",
    "## What's the RMSE in relation to the average ASF value in the test subset?\n",
    "av_asf_test = np.mean(y_test)\n",
    "print(dtr_full_test_rmse/av_asf_test*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scikit-learn has a function called cost_complexity_pruning_path, which gives the effective alpha\n",
    "## of subtrees during pruning and the corresponding impurities (see Lecture Notes - Equation 1.9). \n",
    "## We can use these values of alpha to prune our regression tree\n",
    "path = dtr_full.cost_complexity_pruning_path(X_train, y_train)\n",
    "alpha = path.ccp_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will use these values of alpha and pass it to the ccp_alpha parameter of our DecisionTreeRegressor\n",
    "## By looping over the alphas array, we will find the error on the train and test subsets\n",
    "rmse_train,rmse_test = [],[]\n",
    "for i in alpha:\n",
    "    dtr = DecisionTreeRegressor(ccp_alpha=i, random_state=1234)\n",
    "    dtr.fit(X_train, y_train)\n",
    "    y_train_pred = dtr.predict(X_train)\n",
    "    y_test_pred  = dtr.predict(X_test)\n",
    "    rmse_train.append(np.sqrt(metrics.mean_squared_error(y_train, y_train_pred)))\n",
    "    rmse_test.append(np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plotting alpha vs RMSE (train and test subsets)\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"RMSE (years)\")\n",
    "ax.plot(alpha, rmse_train, label=\"train\")\n",
    "ax.plot(alpha, rmse_test, label=\"test\")\n",
    "plt.xlim([0, 0.7])\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's choose the alpha corresponding to the \"best\" model, i.e., that with the lowest test RMSE\n",
    "index_best_model = np.argmin(rmse_test)\n",
    "alpha_best = alpha[index_best_model]\n",
    "print(alpha_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's then fit a simpler tree using alpha=alpha_best\n",
    "dtr_alpha = DecisionTreeRegressor(ccp_alpha=alpha_best, random_state=1234)\n",
    "dtr_alpha.fit(X_train, y_train)\n",
    "y_test_pred_alpha = dtr_alpha.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the observed vs predicted tree aboveground biomass\n",
    "plt.scatter(y_test, y_test_pred_alpha, s=60, edgecolor='black', c='red', label='data, alpha_best')\n",
    "plt.plot(y_test, y_test, c='blue', label='y=x')\n",
    "plt.xlim([0, 30])\n",
    "plt.ylim([0, 30])\n",
    "plt.xlabel(\"observed age of secondary forests (years)\")\n",
    "plt.ylabel(\"predicted age of secondary forests (years)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now, let's compare with the full tree\n",
    "## Plotting the observed vs predicted tree aboveground biomass\n",
    "plt.scatter(y_test, y_test_pred_full, s=60, edgecolor='black', c='red', label='data, full tree')\n",
    "plt.plot(y_test, y_test, c='blue', label='y=x')\n",
    "plt.xlim([0, 30])\n",
    "plt.ylim([0, 30])\n",
    "plt.xlabel(\"observed age of secondary forests (years)\")\n",
    "plt.ylabel(\"predicted age of secondary forests (years)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the bias-variance decomposition of the test error\n",
    "Use Equation 1.19 to calculate the bias-variance decomposition of the root mean square error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr_alpha_rmse = \"{:.4f}\".format(np.sqrt(metrics.mean_squared_error(y_test, y_test_pred_alpha)))\n",
    "dtr_alpha_bias = \"{:.4f}\".format(np.mean(y_test - y_test_pred_alpha))\n",
    "dtr_alpha_var  = \"{:.4f}\".format(np.var(y_test - y_test_pred_alpha))\n",
    "print(dtr_alpha_rmse, dtr_alpha_bias, dtr_alpha_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr_full_rmse = \"{:.4f}\".format(np.sqrt(metrics.mean_squared_error(y_test, y_test_pred_full)))\n",
    "dtr_full_bias = \"{:.4f}\".format(np.mean(y_test - y_test_pred_full))\n",
    "dtr_full_var  = \"{:.4f}\".format(np.var(y_test - y_test_pred_full))\n",
    "print(dtr_full_rmse, dtr_full_bias, dtr_full_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the importance of each predictor\n",
    "We can use the function permutation_importance to rank the predictors being used to fit any given model. This will give us some insight about the variables that are more important to estimate the aboveground biomass of each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr_alpha_imp = permutation_importance(dtr_alpha, X_train, y_train, n_repeats=100, random_state=1234)\n",
    "sort_imp = dtr_alpha_imp.importances_mean.argsort()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(dtr_alpha_imp.importances[sort_imp].T,\n",
    "           vert=False, labels=X_train.columns[sort_imp])\n",
    "ax.set_title(\"Permutation Importances (train subset)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr_full_imp = permutation_importance(dtr_full, X_train, y_train, n_repeats=100, random_state=1234)\n",
    "sort_imp = dtr_full_imp.importances_mean.argsort()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(dtr_full_imp.importances[sort_imp].T,\n",
    "           vert=False, labels=X_train.columns[sort_imp])\n",
    "ax.set_title(\"Permutation Importances (train subset)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting bagging models\n",
    "The scikit-learn library has a specific function to fit a bagging regression model (see Lecture Notes). The function is called [BaggingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor), and in the link you can learn about the parameters controlling this function.\n",
    "\n",
    "In the code below you'll have to specify two bagging models, using a regression tree as the weak learner :\n",
    "\n",
    "1. Setting 100 estimators, i.e., 100 regression trees\n",
    "2. Setting 600 estimators, i.e., 600 regression trees\n",
    "\n",
    "You can leave the other parameters as default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specifying the bagging models\n",
    "bagg_1 = BaggingRegressor(DecisionTreeRegressor(), n_estimators=100, random_state=1234)\n",
    "bagg_2 = BaggingRegressor(DecisionTreeRegressor(), n_estimators=600, random_state=1234)\n",
    "##\n",
    "## Fitting the bagging models\n",
    "bagg_1.fit(X_train, y_train)\n",
    "bagg_2.fit(X_train, y_train)\n",
    "##\n",
    "## Predicting to the test subset\n",
    "bagg_1.pred = bagg_1.predict(X_test)\n",
    "bagg_2.pred = bagg_2.predict(X_test)\n",
    "##\n",
    "## Estimating the bias, variance and root mean squared error\n",
    "bagg_1_rmse = \"{:.4f}\".format(np.sqrt(metrics.mean_squared_error(y_test, bagg_1.pred)))\n",
    "bagg_1_bias = \"{:.4f}\".format(np.mean(y_test - bagg_1.pred))\n",
    "bagg_1_var  = \"{:.4f}\".format(np.var(y_test - bagg_1.pred))\n",
    "print(bagg_1_rmse, bagg_1_bias, bagg_1_var)\n",
    "bagg_2_rmse = \"{:.4f}\".format(np.sqrt(metrics.mean_squared_error(y_test, bagg_2.pred)))\n",
    "bagg_2_bias = \"{:.4f}\".format(np.mean(y_test - bagg_2.pred))\n",
    "bagg_2_var  = \"{:.4f}\".format(np.var(y_test - bagg_2.pred))\n",
    "print(bagg_2_rmse, bagg_2_bias, bagg_2_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the observed vs predicted tree aboveground biomass\n",
    "plt.scatter(y_test, bagg_1.pred, s=60, edgecolor='black', c='red', label='data, 100 estimators')\n",
    "plt.plot(y_test, y_test, c='blue', label='y=x')\n",
    "plt.xlim([0, 30])\n",
    "plt.ylim([0, 30])\n",
    "plt.xlabel(\"observed age of secondary forests (years)\")\n",
    "plt.ylabel(\"predicted age of secondary forests (years)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the observed vs predicted tree aboveground biomass\n",
    "plt.scatter(y_test, bagg_2.pred, s=60, edgecolor='black', c='red', label='data, 600 estimators')\n",
    "plt.plot(y_test, y_test, c='blue', label='y=x')\n",
    "plt.xlim([0, 30])\n",
    "plt.ylim([0, 30])\n",
    "plt.xlabel(\"observed age of secondary forests (years)\")\n",
    "plt.ylabel(\"predicted age of secondary forests (years)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate variable importance\n",
    "bagg_1_imp = permutation_importance(bagg_1, X_train, y_train, n_repeats=100, random_state=1234)\n",
    "sort_imp = bagg_1_imp.importances_mean.argsort()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(bagg_1_imp.importances[sort_imp].T,\n",
    "           vert=False, labels=X_train.columns[sort_imp])\n",
    "ax.set_title(\"Permutation Importances (train subset)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate variable importance\n",
    "bagg_2_imp = permutation_importance(bagg_2, X_train, y_train, n_repeats=100, random_state=1234)\n",
    "sort_imp = bagg_2_imp.importances_mean.argsort()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(bagg_2_imp.importances[sort_imp].T,\n",
    "           vert=False, labels=X_train.columns[sort_imp])\n",
    "ax.set_title(\"Permutation Importances (train subset)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting boosting models\n",
    "The scikit-learn library has a specific function to fit an AdaBoost regression model (see Lecture Notes). The function is called [AdaBoostRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor), and in the link you can learn about the parameters controlling this function.\n",
    "\n",
    "In the code below you'll have to specify two boosting models, using a regression tree as the weak learner :\n",
    "\n",
    "1. Setting 100 estimators, i.e., 100 regression trees, learning rate = 0.1\n",
    "2. Setting 600 estimators, i.e., 600 regression trees, learning rate = 0.01\n",
    "\n",
    "You can leave the other parameters as default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specifying the boosting model\n",
    "boost_1 = AdaBoostRegressor(DecisionTreeRegressor(), n_estimators=100, learning_rate=0.1, random_state=1234)\n",
    "boost_2 = AdaBoostRegressor(DecisionTreeRegressor(), n_estimators=600, learning_rate=0.01, random_state=1234)\n",
    "##\n",
    "## Fitting the bagging model\n",
    "boost_1.fit(X_train, y_train)\n",
    "boost_2.fit(X_train, y_train)\n",
    "##\n",
    "## Predicting to the test subset\n",
    "boost_1.pred = boost_1.predict(X_test)\n",
    "boost_2.pred = boost_2.predict(X_test)\n",
    "##\n",
    "## Estimating the bias, variance and root mean squared error\n",
    "boost_1_rmse = \"{:.4f}\".format(np.sqrt(metrics.mean_squared_error(y_test, boost_1.pred)))\n",
    "boost_1_bias = \"{:.4f}\".format(np.mean(y_test - boost_1.pred))\n",
    "boost_1_var  = \"{:.4f}\".format(np.var(y_test - boost_1.pred))\n",
    "print(boost_1_rmse, boost_1_bias, boost_1_var)\n",
    "boost_2_rmse = \"{:.4f}\".format(np.sqrt(metrics.mean_squared_error(y_test, boost_2.pred)))\n",
    "boost_2_bias = \"{:.4f}\".format(np.mean(y_test - boost_2.pred))\n",
    "boost_2_var  = \"{:.4f}\".format(np.var(y_test - boost_2.pred))\n",
    "print(boost_2_rmse, boost_2_bias, boost_2_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the observed vs predicted tree aboveground biomass\n",
    "plt.scatter(y_test, boost_1.pred, s=60, edgecolor='black', c='red', label='data, 100 estimators \\nlearning rate=0.1')\n",
    "plt.plot(y_test, y_test, c='blue', label='y=x')\n",
    "plt.xlim([0, 30])\n",
    "plt.ylim([0, 30])\n",
    "plt.xlabel(\"observed age of secondary forests (years)\")\n",
    "plt.ylabel(\"predicted age of secondary forests (years)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the observed vs predicted tree aboveground biomass\n",
    "plt.scatter(y_test, boost_2.pred, s=60, edgecolor='black', c='red', label='data, 600 estimators \\nlearning rate=0.01')\n",
    "plt.plot(y_test, y_test, c='blue', label='y=x')\n",
    "plt.xlim([0, 30])\n",
    "plt.ylim([0, 30])\n",
    "plt.xlabel(\"observed age of secondary forests (years)\")\n",
    "plt.ylabel(\"predicted age of secondary forests (years)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Calculate variable importance\n",
    "boost_1_imp = permutation_importance(boost_1, X_train, y_train, n_repeats=100, random_state=1234)\n",
    "sort_imp = boost_1_imp.importances_mean.argsort()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(boost_1_imp.importances[sort_imp].T,\n",
    "           vert=False, labels=X_train.columns[sort_imp])\n",
    "ax.set_title(\"Permutation Importances (train subset)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate variable importance\n",
    "boost_2_imp = permutation_importance(boost_2, X_train, y_train, n_repeats=100, random_state=1234)\n",
    "sort_imp = boost_2_imp.importances_mean.argsort()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(boost_2_imp.importances[sort_imp].T,\n",
    "           vert=False, labels=X_train.columns[sort_imp])\n",
    "ax.set_title(\"Permutation Importances (train subset)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And the winner is...\n",
    "Let's then compare the results from all fitted algorithms: regression trees, bagging and boosting.\n",
    "\n",
    "You'll need to make a table with the RMSE, bias and variance values that were calculated for all fitted models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, create a list with the elements you want to display\n",
    "table_error = [['algorithm', 'RMSE', 'bias', 'variance'],  ## headers\n",
    "               ['dtr_full', dtr_full_rmse, dtr_full_bias, dtr_full_var], \n",
    "               ['dtr_alpha', dtr_alpha_rmse, dtr_alpha_bias, dtr_alpha_var], \n",
    "               ['bagg_1', bagg_1_rmse, bagg_1_bias, bagg_1_var],\n",
    "               ['bagg_2', bagg_2_rmse, bagg_2_bias, bagg_2_var],\n",
    "               ['boost_1', boost_1_rmse, boost_1_bias, boost_1_var],\n",
    "               ['boost_2', boost_2_rmse, boost_2_bias, boost_2_var]]\n",
    "##\n",
    "## Then, use the tabulate function\n",
    "print(tabulate(table_error, headers='firstrow', tablefmt='fancy_grid'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
